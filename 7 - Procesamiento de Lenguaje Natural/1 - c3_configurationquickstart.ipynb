{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_T-8Ff0mc6y"
   },
   "source": [
    "# **Como usar NLTK en Google Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1581907027323,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "6fuk75qphe8f",
    "outputId": "2f8f3a9a-02a5-406a-a34e-9b4a8c52d4d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\Tarto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seleccionar download [d], luego descargar el recurso de nombre \"book\"\n",
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARpUziJ6m4y-"
   },
   "source": [
    "# **Expresiones Regulares**\n",
    "\n",
    "\n",
    "*   Constituyen un lenguaje estandarizado para definir cadenas de búsqueda de texto.\n",
    "*   Libreria de operaciones con  expresiones regulares de Python [re](https://docs.python.org/3/library/re.html)\n",
    "*   Reglas para escribir expresiones regulares [Wiki](https://es.wikipedia.org/wiki/Expresión_regular)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12358,
     "status": "ok",
     "timestamp": 1581907039224,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "OYt70VjHn4DT",
    "outputId": "8ead6770-41a7-46e9-f135-94ea9a607a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
      "6030\n"
     ]
    }
   ],
   "source": [
    "# Corpus en español: https://mailman.uib.no/public/corpora/2007-October/005448.html\n",
    "import re\n",
    "corpus = nltk.corpus.cess_esp.sents() \n",
    "print(corpus)\n",
    "print(len(corpus))\n",
    "# Con esto se puede visualizar la estructura, como es con [[]] son \"Listas de lista\", y contiene 6030 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18266,
     "status": "ok",
     "timestamp": 1581907045142,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "HV7LSLQZqIHf",
    "outputId": "d8dbd63c-83b4-49ae-9446-57db254882e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n",
      "192686\n"
     ]
    }
   ],
   "source": [
    "flatten = [w for l in corpus for w in l]\n",
    "# for l in corpus:\n",
    "    # for w in l:\n",
    "\n",
    "print(flatten[:20])  # Para conocer la Sintaxis\n",
    "print(len(flatten))\n",
    "# Se transforma la lista de lista para crear UNA SOLA LISTA con todas las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJqD6PIxr3EO"
   },
   "source": [
    "### **Estructura de la funcion re.search()**\n",
    "```\n",
    "# Determina si el patron de búsqueda p esta contenido en la cadena s\n",
    "re.seach(p, s)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18257,
     "status": "ok",
     "timestamp": 1581907045143,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "Xo5mGuUUrs2Z",
    "outputId": "d0dbb437-0dff-4bcd-f54a-7034bc103cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']\n",
      "14814\n"
     ]
    }
   ],
   "source": [
    "# Meta-caracteres básicos\n",
    "# w = Objeto Palabra,  Flatten = Objeto lista con palabras     re.search(p, s) p = Patros busqueda    s = Cadena de Texto\n",
    "arr = [w for w in flatten if re.search('es', w)]\n",
    "# Obtiene todas las palabras que contrangan 'es' en cualquier ubicacion\n",
    "print(arr[:5])\n",
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18446,
     "status": "ok",
     "timestamp": 1581907045341,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "UB4Jmzhkutj_",
    "outputId": "c9d4e11d-25b8-4d49-f095-ae778bd37f15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los signos son \"Expresiones regulares\", cada una tiene su regla.\n",
    "# en este caso, terminar con \"$\" habla de la terminacion \"e s\"\n",
    "# Para el inicio? Como se hace? Simple, con \"^\" al comienzo, ej: '^es'\n",
    "arr = [w for w in flatten if re.search('es$', w)]\n",
    "arr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grupo', 'hoy', 'gas', 'gas', 'intervendrá']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambien se puede complejizar, como el inicio con 3 letras diferentes.\n",
    "arr = [w for w in flatten if re.search('^[ghi]', w)]\n",
    "arr[:5]\n",
    "# (): secuencias\n",
    "# * : repetir 0 o más veces\n",
    "# + : repetir 1 o más veces     ex: '^(no)*'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18438,
     "status": "ok",
     "timestamp": 1581907045343,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "W4l-5006wDGT",
    "outputId": "43e61001-9c6d-480f-fca5-a31ef5fb085a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tajantes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [w for w in flatten if re.search('^..j..t..$', w)]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18637,
     "status": "ok",
     "timestamp": 1581907045553,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "BsOTXCVHwbbA",
    "outputId": "79329075-8d65-4615-85eb-d0df94996b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['golf', 'golf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rangos [a-z], [A-Z], [0-9]\n",
    "arr = [w for w in flatten if re.search('^[ghi][mno][jlk][def]$', w)]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18814,
     "status": "ok",
     "timestamp": 1581907045742,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "63AMmVplxmoX",
    "outputId": "b8caccc4-b010-49e2-9943-8fa64d3040bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'grupo',\n",
       " 'estatal',\n",
       " 'Electricité_de_France',\n",
       " '-Fpa-',\n",
       " 'EDF',\n",
       " '-Fpt-',\n",
       " 'anunció',\n",
       " 'hoy',\n",
       " ',']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clausuras *, * (Kleene closures)\n",
    "arr = [w for w in flatten if re.search('^(no)*', w)]\n",
    "arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19284,
     "status": "ok",
     "timestamp": 1581907046220,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "tHH8YgfTy_uh",
    "outputId": "09201656-14fe-427d-81f6-2a4b484f1582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['norte',\n",
       " 'no',\n",
       " 'no',\n",
       " 'noche',\n",
       " 'no',\n",
       " 'no',\n",
       " 'gobierno',\n",
       " 'notificación',\n",
       " 'Unión_Fenosa_Inversiones',\n",
       " 'italiano']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [w for w in flatten if re.search('(no)+', w)]\n",
    "arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBWZl5Sf0ID0"
   },
   "source": [
    "# **Normalización de Texto** (como aplicación de las expresiones regulares)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-3Gr4tRj-Y_"
   },
   "source": [
    "## **Tokenización:** Es el proceso mediante el cual se sub-divide una cadena de texto en unidades linguísticas minimas (palabras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19276,
     "status": "ok",
     "timestamp": 1581907046221,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "vNwS7E8GiuDS",
    "outputId": "d20eac6c-d586-4c91-e172-30ade7c2d1b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuando sea el rey del mundo  (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. \n",
      "            Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera visualizar en su cabeza ...\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\" Cuando sea el rey del mundo  (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. \n",
    "            Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera visualizar en su cabeza ...\"\"\"\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19269,
     "status": "ok",
     "timestamp": 1581907046222,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "BkS7xN_ulYP_",
    "outputId": "e2cf2948-d06a-4422-88da-a647b3043ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', '', 'preocuparme', 'por', 'estas', 'bobadas.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
     ]
    }
   ],
   "source": [
    "# Caso 1: tokenizacion más simple: por espacios vacios!\n",
    "# r = RAW       texto = variable a estudiar\n",
    "print(re.split(r' ', texto))\n",
    "# Separa todo utilizando los espacios(' ') como \"separador\"\n",
    "# Pero aun no esta refinado, hay mucho ruido, como ', ' o '(imaginaba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19263,
     "status": "ok",
     "timestamp": 1581907046224,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "byIH7y5QllB5",
    "outputId": "be1f0c5c-43da-4978-b0b0-b5263582b2de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años,', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
     ]
    }
   ],
   "source": [
    "# Caso 2: tokenización usando expresiones regulares (regex)\n",
    "print(re.split(r'[ \\t\\n]+', texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19256,
     "status": "ok",
     "timestamp": 1581907046225,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "hliGNzLK1MMd",
    "outputId": "f4ebc913-271a-46d7-ccb4-59cec3851c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
     ]
    }
   ],
   "source": [
    "# RegEx reference: \\W -> all characters other than letters, digits or underscore\n",
    "print(re.split(r'[ \\W\\t\\n]+', texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyflLu-z3RXk"
   },
   "source": [
    "## **Tokenizador de NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19248,
     "status": "ok",
     "timestamp": 1581907046227,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "uvHYj9O_1wb4",
    "outputId": "82d4e9bb-4da3-478f-e906-e2b4d6e00c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
     ]
    }
   ],
   "source": [
    "# nuestra antigua regex no funciona en este caso: \n",
    "texto = 'En los E.U. esa postal vale $15.50 ...'\n",
    "print(re.split(r'[ \\W\\t\\n]+', texto))\n",
    "# Aqui se puede observar que separa de manera incorrecta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19240,
     "status": "ok",
     "timestamp": 1581907046228,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "tLDA7ARv3my9",
    "outputId": "17e6c034-61a5-4b65-b8b0-a08d7115b490"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'''(?x)                 # Forma de iniciar \"Modo verbos\"\n",
    "              (?:[A-Z]\\.)+         # Incluye abreviaciones como por ejemplo: U.S.A.\n",
    "              | \\w+(?:-\\w+)*       # Incluye a las palabras con un guion [-] interno\n",
    "              | \\$?\\d+(?:\\.\\d+)?%? # Incluye dinero y porcentajes, e.g. $12.40, 82%\n",
    "              | \\.\\.\\.             # Agrega los puntos suspensivos\n",
    "              | [][.,;\"'?():-_`]   # Tokens de separacion; como por ejemplo: ], [\n",
    "'''\n",
    "nltk.regexp_tokenize(texto, pattern)  # Funcion Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGKfeofg43c6"
   },
   "source": [
    "## **Lematización:** Proceso para encontrar la raíz linguística de una palabra\n",
    "\n",
    "*   Derivación (stemming) : lematización simple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1581907509908,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "TJ7AKL-y6XO9",
    "outputId": "ae0ccf92-f289-4c40-90a6-f0d106395ae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivación simple\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1581907522176,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "e9Ms30n8-cC2",
    "outputId": "814b5729-ee14-431e-ca12-02d6c0d7876f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trabaj'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = SnowballStemmer('spanish')\n",
    "stem.stem('trabajando')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHIQIyDL-O68"
   },
   "outputs": [],
   "source": [
    "# Lematización\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1581907541479,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUaNNj72-A-ZLpzeOCw1kooH47L4wGZaSy6fUG=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "ezzA3MpTAu3y",
    "outputId": "00883200-170d-4af5-c283-80fb349de6da"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tarto/nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tarto/nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-68bed1523f57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trabajando'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tarto/nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\anaconda3\\\\envs\\\\ml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tarto\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemm.lemmatize('trabajando')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6_T-8Ff0mc6y",
    "ARpUziJ6m4y-",
    "MBWZl5Sf0ID0",
    "0-3Gr4tRj-Y_",
    "lyflLu-z3RXk"
   ],
   "name": "[C3]_configuration+quickstart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
